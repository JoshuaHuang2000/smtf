import asyncio
from playwright.async_api import async_playwright
import os
import random
import re  # åˆ«å¿˜äº†å¯¼å…¥ re


class Harvester:
    def __init__(self, user_data_dir="browser_data", headless=False):
        self.user_data_dir = user_data_dir
        self.headless = headless

    async def harvest(self, max_posts=5):
        return await self.harvest_x_timeline(max_posts)

    async def harvest_x_timeline(self, max_posts=5):
        print(f"[*] [X] Attempting to connect to LOCAL Chrome (Port 9222)...")

        async with async_playwright() as p:
            try:
                # 1. è¿æ¥æœ¬åœ° Chrome (å¼ºåˆ¶ IPv4)
                browser = await p.chromium.connect_over_cdp("http://127.0.0.1:9222")
                default_context = browser.contexts[0]
            except Exception as e:
                print(f"    [!] Connection failed: {e}")
                print(
                    r"    ğŸ‘‰ è¯·å…ˆåœ¨ç»ˆç«¯è¿è¡Œ: /Applications/Google\ Chrome.app/Contents/MacOS/Google\ Chrome --remote-debugging-port=9222 --user-data-dir='/tmp/chrome_dev_session'")
                return []

            # 2. å¯»æ‰¾ X æ ‡ç­¾é¡µ
            page = None
            pages = default_context.pages

            for p_tab in pages:
                if "x.com" in p_tab.url or "twitter.com" in p_tab.url:
                    page = p_tab
                    print("    -> Found existing X tab, reusing...")
                    # [ä¼˜åŒ–] æ³¨é‡Šæ‰è¿™å°±ä¸ä¼šå¼ºåˆ¶åˆ‡å±äº†ï¼Œè®©å®ƒåœ¨åå°é»˜é»˜è·‘
                    # await page.bring_to_front()
                    break

            if not page:
                print("    -> No X tab found, opening new one...")
                page = await default_context.new_page()
                await page.goto("https://x.com/home")

            # [æ ¸å¿ƒé»‘ç§‘æŠ€] CDP å¼ºåˆ¶ä¼ªè£…å¯è§æ€§
            try:
                # ä½¿ç”¨ default_context åˆ›å»ºä¼šè¯æ›´ç¨³å¥
                client = await default_context.new_cdp_session(page)
                # å‘Šè¯‰ Chrome: "å˜¿ï¼Œä½ å°±åœ¨å±å¹•æœ€ä¸­é—´ï¼Œç”¨æˆ·æ­£ç›¯ç€ä½ çœ‹å‘¢"
                await client.send("Emulation.setFocusEmulationEnabled", {"enabled": True})
                await client.send("Emulation.setCPUThrottlingRate", {"rate": 1})
                await client.send("Emulation.setAutoDarkModeOverride", {"enabled": False})
                print("    -> [Stealth] CDP visibility spoofing active.")
            except Exception as e:
                print(f"    [!] CDP Warning: {e}")

            # 3. åˆ·æ–°ä¸åŠ è½½
            print("    -> Refreshing timeline...")
            try:
                await page.reload(wait_until="domcontentloaded")
                await page.wait_for_selector('[data-testid="tweet"]', state="visible", timeout=15000)
                print("    -> Timeline ready.")
            except:
                print("    [!] Timeline timeout. Please check Chrome window manually.")
                # æ–­å¼€è¿æ¥ï¼Œä¸æ€è¿›ç¨‹
                await browser.close()
                return []

            # 4. æŠ“å–æµç¨‹
            print("    -> Scraping timeline...")
            posts_data = []
            seen_ids = set()
            os.makedirs("assets/images", exist_ok=True)

            for i in range(3):
                tweets = await page.locator('[data-testid="tweet"]').all()
                print(f"       (Scroll {i + 1}) Visible tweets: {len(tweets)}")

                for tweet in tweets:
                    if len(posts_data) >= max_posts: break

                    try:
                        extracted_id = None
                        final_url = ""

                        # ID / URL æå–
                        links = await tweet.locator('a[href*="/status/"]').all()
                        for link in links:
                            href = await link.get_attribute("href")
                            if "/status/" in href:
                                parts = href.split("/status/")
                                if len(parts) > 1:
                                    possible_id = parts[1].split("/")[0].split("?")[0]
                                    if possible_id.isdigit():
                                        extracted_id = f"x_{possible_id}"
                                        final_url = f"https://x.com{href}"
                                        break

                        text = await tweet.inner_text()
                        clean_text = text.replace("\n", " ").strip()
                        if not extracted_id: extracted_id = f"x_hash_{hash(clean_text)}"

                        if extracted_id in seen_ids: continue

                        # å›¾ç‰‡ä¸‹è½½ (ç›´æ¥å¤ç”¨ Page Request)
                        image_local_path = None
                        photo_divs = await tweet.locator('[data-testid="tweetPhoto"] img').all()

                        if photo_divs:
                            img_src = await photo_divs[0].get_attribute("src")
                            if img_src:
                                # æ›¿æ¢ä¸ºåŸå›¾
                                if "name=" in img_src:
                                    high_res_src = re.sub(r"name=\w+", "name=orig", img_src)
                                else:
                                    high_res_src = img_src

                                ext = "jpg"
                                if "format=png" in high_res_src: ext = "png"
                                filename = f"assets/images/{extracted_id}.{ext}"

                                try:
                                    # åˆ©ç”¨å½“å‰é¡µé¢çš„ Context ä¸‹è½½ï¼Œæœ€å®‰å…¨
                                    response = await page.request.get(high_res_src)
                                    if response.status == 200:
                                        with open(filename, "wb") as f:
                                            f.write(await response.body())
                                        image_local_path = filename
                                except:
                                    pass

                        if len(clean_text) > 20 or image_local_path:
                            seen_ids.add(extracted_id)
                            posts_data.append({
                                "id": extracted_id,
                                "text": clean_text[:500],
                                "url": final_url,
                                "image_path": image_local_path
                            })

                    except Exception:
                        continue

                if len(posts_data) >= max_posts: break

                # éšæœºæ»šåŠ¨æ¨¡æ‹Ÿ
                await page.mouse.wheel(0, random.randint(800, 1500))
                await asyncio.sleep(random.uniform(2.0, 4.0))

            print(f"    -> Harvested {len(posts_data)} posts.")

            # [é‡è¦] åªæ˜¯æ–­å¼€è¿æ¥ï¼ŒChrome ä¾ç„¶åœ¨åå°è¿è¡Œ
            await browser.close()
            print("    -> Disconnected (Chrome stays open).")

            return posts_data


if __name__ == "__main__":
    h = Harvester()
    asyncio.run(h.harvest(3))
